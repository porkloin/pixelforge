//! Image utilities for creating and uploading video frames.
//!
//! This module provides utilities for creating Vulkan images suitable for video encoding.
//! and uploading YUV data to them.

use crate::encoder::{BitDepth, Codec, PixelFormat};
use crate::error::{PixelForgeError, Result};
use crate::vulkan::VideoContext;
use ash::vk;

/// An image on the GPU ready for video encoding.
///
/// This struct owns a Vulkan image with NV12/P010 format (YUV420) or
/// 2-plane semi-planar YUV444 format and provides methods to upload YUV data to it.
/// The image can be passed directly to the encoder.
pub struct InputImage {
    context: VideoContext,
    image: vk::Image,
    memory: vk::DeviceMemory,
    staging_buffer: vk::Buffer,
    staging_memory: vk::DeviceMemory,
    staging_size: usize,
    /// Kept for automatic cleanup via Drop.
    #[allow(dead_code)]
    command_pool: vk::CommandPool,
    command_buffer: vk::CommandBuffer,
    fence: vk::Fence,
    width: u32,
    height: u32,
    bit_depth: BitDepth,
    pixel_format: PixelFormat,
}

impl InputImage {
    /// Create a new input image for video encoding.
    ///
    /// Creates an image suitable for use as input to the video encoder.
    /// For YUV420: NV12 (8-bit) or P010 (10-bit) format.
    /// For YUV444: 2-plane semi-planar format (8-bit or 10-bit).
    /// The image is allocated in device-local memory for optimal performance.
    ///
    /// # Arguments
    /// * `context` - The Vulkan video context
    /// * `codec` - The video codec (H.264/H.265)
    /// * `width` - Image width in pixels
    /// * `height` - Image height in pixels
    /// * `bit_depth` - Bit depth for the image (8-bit or 10-bit)
    /// * `pixel_format` - Pixel format (YUV420 or YUV444)
    pub fn new(
        context: VideoContext,
        codec: Codec,
        width: u32,
        height: u32,
        bit_depth: BitDepth,
        pixel_format: PixelFormat,
    ) -> Result<Self> {
        let device = context.device();

        // Determine H.264 profile IDC based on pixel format.
        let h264_profile_idc = if pixel_format == PixelFormat::Yuv444 {
            ash::vk::native::StdVideoH264ProfileIdc_STD_VIDEO_H264_PROFILE_IDC_HIGH_444_PREDICTIVE
        } else {
            ash::vk::native::StdVideoH264ProfileIdc_STD_VIDEO_H264_PROFILE_IDC_HIGH
        };

        // Create video profile info for the image creation.
        let mut h264_profile = vk::VideoEncodeH264ProfileInfoKHR::default()
            .std_profile_idc(h264_profile_idc);
        let mut h265_profile = vk::VideoEncodeH265ProfileInfoKHR::default()
            .std_profile_idc(ash::vk::native::StdVideoH265ProfileIdc_STD_VIDEO_H265_PROFILE_IDC_MAIN);

        let mut profile_info = vk::VideoProfileInfoKHR::default()
            .chroma_subsampling(pixel_format.into())
            .luma_bit_depth(bit_depth.into())
            .chroma_bit_depth(bit_depth.into());

        match codec {
            Codec::H264 => {
                profile_info = profile_info
                    .video_codec_operation(vk::VideoCodecOperationFlagsKHR::ENCODE_H264);
                profile_info.p_next = &mut h264_profile as *mut _ as *mut std::ffi::c_void;
            }
            Codec::H265 => {
                profile_info = profile_info
                    .video_codec_operation(vk::VideoCodecOperationFlagsKHR::ENCODE_H265);
                profile_info.p_next = &mut h265_profile as *mut _ as *mut std::ffi::c_void;
            }
            _ => return Err(PixelForgeError::InvalidInput("Unsupported codec".to_string())),
        }

        let mut profile_list = vk::VideoProfileListInfoKHR::default()
            .profiles(std::slice::from_ref(&profile_info));

        // Select format based on pixel format and bit depth.
        // Use 2-plane semi-planar formats for both YUV420 and YUV444.
        let format = match (pixel_format, bit_depth) {
            (PixelFormat::Yuv420, BitDepth::Eight) => vk::Format::G8_B8R8_2PLANE_420_UNORM,
            (PixelFormat::Yuv420, BitDepth::Ten) => {
                vk::Format::G10X6_B10X6R10X6_2PLANE_420_UNORM_3PACK16
            }
            (PixelFormat::Yuv444, BitDepth::Eight) => vk::Format::G8_B8R8_2PLANE_444_UNORM,
            (PixelFormat::Yuv444, BitDepth::Ten) => {
                vk::Format::G10X6_B10X6R10X6_2PLANE_444_UNORM_3PACK16
            }
            _ => {
                return Err(PixelForgeError::InvalidInput(format!(
                    "Unsupported pixel format / bit depth combination: {:?} / {:?}",
                    pixel_format, bit_depth
                )));
            }
        };

        // Determine sharing mode and queue families.
        let mut queue_families = vec![context.transfer_queue_family()];
        if let Some(encode_family) = context.video_encode_queue_family() {
            if encode_family != context.transfer_queue_family() {
                queue_families.push(encode_family);
            }
        }

        let sharing_mode = if queue_families.len() > 1 {
            vk::SharingMode::CONCURRENT
        } else {
            vk::SharingMode::EXCLUSIVE
        };

        // Create the image.
        let mut image_create_info = vk::ImageCreateInfo::default()
            .image_type(vk::ImageType::TYPE_2D)
            .format(format)
            .extent(vk::Extent3D {
                width,
                height,
                depth: 1,
            })
            .mip_levels(1)
            .array_layers(1)
            .samples(vk::SampleCountFlags::TYPE_1)
            .tiling(vk::ImageTiling::OPTIMAL)
            .usage(
                vk::ImageUsageFlags::TRANSFER_DST
                    | vk::ImageUsageFlags::TRANSFER_SRC
                    | vk::ImageUsageFlags::SAMPLED
                    | vk::ImageUsageFlags::VIDEO_ENCODE_SRC_KHR,
            )
            .sharing_mode(sharing_mode)
            .queue_family_indices(&queue_families)
            .initial_layout(vk::ImageLayout::UNDEFINED);

        image_create_info.p_next = &mut profile_list as *mut _ as *mut std::ffi::c_void;

        let image = unsafe { device.create_image(&image_create_info, None) }
            .map_err(|e| PixelForgeError::ResourceCreation(format!("image creation: {}", e)))?;

        // Get memory requirements and allocate.
        let mem_requirements = unsafe { device.get_image_memory_requirements(image) };

        let memory_type_index = context
            .find_memory_type(
                mem_requirements.memory_type_bits,
                vk::MemoryPropertyFlags::DEVICE_LOCAL,
            )
            .ok_or_else(|| {
                PixelForgeError::MemoryAllocation("No suitable memory type for image".to_string())
            })?;

        let alloc_info = vk::MemoryAllocateInfo::default()
            .allocation_size(mem_requirements.size)
            .memory_type_index(memory_type_index);

        let memory = unsafe { device.allocate_memory(&alloc_info, None) }
            .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe { device.bind_image_memory(image, memory, 0) }
            .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        // Create staging buffer for uploads.
        // Size depends on pixel format and bit depth.
        let staging_size = pixel_format.frame_size(width, height)
            * match bit_depth {
                BitDepth::Eight => 1,
                BitDepth::Ten => 2, // 2 bytes per sample for 10-bit
            };
        let buffer_create_info = vk::BufferCreateInfo::default()
            .size(staging_size as vk::DeviceSize)
            .usage(vk::BufferUsageFlags::TRANSFER_SRC)
            .sharing_mode(vk::SharingMode::EXCLUSIVE);

        let staging_buffer = unsafe { device.create_buffer(&buffer_create_info, None) }
            .map_err(|e| PixelForgeError::ResourceCreation(format!("buffer creation: {}", e)))?;

        let staging_mem_requirements =
            unsafe { device.get_buffer_memory_requirements(staging_buffer) };

        let staging_memory_type = context
            .find_memory_type(
                staging_mem_requirements.memory_type_bits,
                vk::MemoryPropertyFlags::HOST_VISIBLE | vk::MemoryPropertyFlags::HOST_COHERENT,
            )
            .ok_or_else(|| {
                PixelForgeError::MemoryAllocation(
                    "No suitable memory type for staging buffer".to_string(),
                )
            })?;

        let staging_alloc_info = vk::MemoryAllocateInfo::default()
            .allocation_size(staging_mem_requirements.size)
            .memory_type_index(staging_memory_type);

        let staging_memory = unsafe { device.allocate_memory(&staging_alloc_info, None) }
            .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe { device.bind_buffer_memory(staging_buffer, staging_memory, 0) }
            .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        // Create command pool and buffer for transfers.
        let pool_info = vk::CommandPoolCreateInfo::default()
            .queue_family_index(context.transfer_queue_family())
            .flags(vk::CommandPoolCreateFlags::RESET_COMMAND_BUFFER);

        let command_pool = unsafe { device.create_command_pool(&pool_info, None) }
            .map_err(|e| PixelForgeError::CommandBuffer(e.to_string()))?;

        let alloc_info = vk::CommandBufferAllocateInfo::default()
            .command_pool(command_pool)
            .level(vk::CommandBufferLevel::PRIMARY)
            .command_buffer_count(1);

        let command_buffers = unsafe { device.allocate_command_buffers(&alloc_info) }
            .map_err(|e| PixelForgeError::CommandBuffer(e.to_string()))?;
        let command_buffer = command_buffers[0];

        // Create fence for synchronization.
        let fence_info = vk::FenceCreateInfo::default();
        let fence = unsafe { device.create_fence(&fence_info, None) }
            .map_err(|e| PixelForgeError::ResourceCreation(format!("fence creation: {}", e)))?;

        Ok(Self {
            context,
            image,
            memory,
            staging_buffer,
            staging_memory,
            staging_size,
            command_pool,
            command_buffer,
            fence,
            width,
            height,
            bit_depth,
            pixel_format,
        })
    }

    /// Upload YUV420 (I420) data to this image.
    ///
    /// The data is expected in I420 format (Y plane, then U plane, then V plane).
    /// This method converts it to NV12 (8-bit) or P010 (10-bit) format and uploads
    /// it to the GPU.
    ///
    /// For 10-bit images, 8-bit input data is expanded to 10-bit by left-shifting
    /// each sample by 6 bits (filling in the 6 lower bits with the high bits).
    pub fn upload_yuv420(&mut self, yuv_data: &[u8]) -> Result<()> {
        let expected_size = (self.width * self.height * 3 / 2) as usize;
        if yuv_data.len() < expected_size {
            return Err(PixelForgeError::InvalidInput(format!(
                "YUV data too small: expected {} bytes, got {}",
                expected_size,
                yuv_data.len()
            )));
        }

        let device = self.context.device();
        let width = self.width as usize;
        let height = self.height as usize;
        let y_size = width * height;

        // Map staging buffer and convert I420 to NV12/P010.
        let data_ptr = unsafe {
            device.map_memory(
                self.staging_memory,
                0,
                self.staging_size as vk::DeviceSize,
                vk::MemoryMapFlags::empty(),
            )
        }
        .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe {
            match self.bit_depth {
                BitDepth::Eight => {
                    // 8-bit NV12 format
                    let dst =
                        std::slice::from_raw_parts_mut(data_ptr as *mut u8, self.staging_size);

                    // Copy Y plane directly.
                    dst[..y_size].copy_from_slice(&yuv_data[..y_size]);

                    // Interleave U and V planes to create NV12 UV plane.
                    let u_plane = &yuv_data[y_size..y_size + y_size / 4];
                    let v_plane = &yuv_data[y_size + y_size / 4..];
                    for i in 0..y_size / 4 {
                        dst[y_size + i * 2] = u_plane[i];
                        dst[y_size + i * 2 + 1] = v_plane[i];
                    }
                }
                BitDepth::Ten => {
                    // 10-bit P010 format: each sample is stored as 16-bit little-endian
                    // with the 10-bit value in the high bits (bits 6-15), and bits 0-5 as padding.
                    //
                    // To convert 8-bit (0-255) to 10-bit (0-1023):
                    // - Multiply by ~4 to scale the range: (val * 1023) / 255 â‰ˆ val * 4
                    // - This is equivalent to: (val << 2) to get 10-bit value
                    // - Then shift left by 6 to place in P010 format: (val << 2) << 6 = val << 8
                    //
                    // For better precision, we can also fill in the 2 LSBs of the 10-bit value
                    // using the 2 MSBs of the original 8-bit value: (val << 8) | (val >> 6 << 6)
                    // Simplified: val << 8 puts the 8-bit value in bits 8-15, which is correct for P010.
                    let dst =
                        std::slice::from_raw_parts_mut(data_ptr as *mut u16, self.staging_size / 2);

                    // Convert and copy Y plane (expand 8-bit to 10-bit P010).
                    for i in 0..y_size {
                        let val = yuv_data[i] as u16;
                        // Shift left by 8 to place 8-bit value in bits 8-15 (P010 format).
                        // This effectively scales 0-255 to 0-65280 with proper bit alignment.
                        dst[i] = val << 8;
                    }

                    // Interleave U and V planes to create P010 UV plane.
                    let u_plane = &yuv_data[y_size..y_size + y_size / 4];
                    let v_plane = &yuv_data[y_size + y_size / 4..];
                    for i in 0..y_size / 4 {
                        let u_val = u_plane[i] as u16;
                        let v_val = v_plane[i] as u16;
                        dst[y_size + i * 2] = u_val << 8;
                        dst[y_size + i * 2 + 1] = v_val << 8;
                    }
                }
            }

            device.unmap_memory(self.staging_memory);
        }

        // Record and submit copy commands.
        self.copy_staging_to_image()?;

        Ok(())
    }

    /// Upload NV12 data directly to this image.
    ///
    /// The data is expected in NV12 format (Y plane, then interleaved UV plane).
    pub fn upload_nv12(&mut self, nv12_data: &[u8]) -> Result<()> {
        let expected_size = (self.width * self.height * 3 / 2) as usize;
        if nv12_data.len() < expected_size {
            return Err(PixelForgeError::InvalidInput(format!(
                "NV12 data too small: expected {} bytes, got {}",
                expected_size,
                nv12_data.len()
            )));
        }

        let device = self.context.device();

        // Map staging buffer and copy directly.
        let data_ptr = unsafe {
            device.map_memory(
                self.staging_memory,
                0,
                self.staging_size as vk::DeviceSize,
                vk::MemoryMapFlags::empty(),
            )
        }
        .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe {
            let dst = std::slice::from_raw_parts_mut(data_ptr as *mut u8, self.staging_size);
            dst.copy_from_slice(&nv12_data[..self.staging_size]);
            device.unmap_memory(self.staging_memory);
        }

        // Record and submit copy commands.
        self.copy_staging_to_image()?;

        Ok(())
    }

    /// Upload native 10-bit YUV420 (I420 10-bit) data to this image.
    ///
    /// The data is expected in yuv420p10le format:
    /// - Y plane: width * height * 2 bytes (16-bit little-endian samples)
    /// - U plane: (width/2) * (height/2) * 2 bytes (16-bit little-endian samples)
    /// - V plane: (width/2) * (height/2) * 2 bytes (16-bit little-endian samples)
    ///
    /// The 10-bit values are stored in the lower 10 bits of each 16-bit sample.
    /// This method converts them to P010 format (10-bit in upper bits) for the encoder.
    ///
    /// This method is only valid for 10-bit InputImage instances.
    pub fn upload_yuv420_10bit(&mut self, yuv_data: &[u8]) -> Result<()> {
        if self.bit_depth != BitDepth::Ten {
            return Err(PixelForgeError::InvalidInput(
                "upload_yuv420_10bit can only be used with 10-bit InputImage".to_string(),
            ));
        }

        // 10-bit YUV420 size: Y (w*h*2) + U (w*h/4*2) + V (w*h/4*2) = w*h*3 bytes
        let expected_size = (self.width * self.height * 3) as usize;
        if yuv_data.len() < expected_size {
            return Err(PixelForgeError::InvalidInput(format!(
                "YUV420 10-bit data too small: expected {} bytes, got {}",
                expected_size,
                yuv_data.len()
            )));
        }

        let device = self.context.device();
        let width = self.width as usize;
        let height = self.height as usize;
        let y_size = width * height; // Number of Y samples

        // Map staging buffer.
        let data_ptr = unsafe {
            device.map_memory(
                self.staging_memory,
                0,
                self.staging_size as vk::DeviceSize,
                vk::MemoryMapFlags::empty(),
            )
        }
        .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe {
            // Source is yuv420p10le (10-bit in lower bits of 16-bit words)
            // Destination is P010 (10-bit in upper bits of 16-bit words)
            let src =
                std::slice::from_raw_parts(yuv_data.as_ptr() as *const u16, expected_size / 2);
            let dst = std::slice::from_raw_parts_mut(data_ptr as *mut u16, self.staging_size / 2);

            // Convert and copy Y plane (shift 10-bit values to upper bits for P010).
            for i in 0..y_size {
                // yuv420p10le has 10-bit value in bits 0-9, P010 needs it in bits 6-15
                dst[i] = src[i] << 6;
            }

            // Convert and interleave U and V planes to create P010 UV plane.
            // Source U plane starts at y_size, V plane at y_size + y_size/4
            let u_offset = y_size;
            let v_offset = y_size + y_size / 4;
            for i in 0..y_size / 4 {
                let u_val = src[u_offset + i];
                let v_val = src[v_offset + i];
                dst[y_size + i * 2] = u_val << 6;
                dst[y_size + i * 2 + 1] = v_val << 6;
            }

            device.unmap_memory(self.staging_memory);
        }

        // Record and submit copy commands.
        self.copy_staging_to_image()?;

        Ok(())
    }

    /// Upload YUV444 (planar) data to an external image.
    pub fn upload_yuv444_to(&mut self, target_image: vk::Image, yuv_data: &[u8]) -> Result<()> {
        if self.pixel_format != PixelFormat::Yuv444 {
            return Err(PixelForgeError::InvalidInput(
                "upload_yuv444_to can only be used with YUV444 InputImage".to_string(),
            ));
        }

        let plane_size = (self.width * self.height) as usize;
        let expected_size = plane_size * 3; // Y + U + V at full resolution
        if yuv_data.len() < expected_size {
            return Err(PixelForgeError::InvalidInput(format!(
                "YUV444 data too small: expected {} bytes, got {}",
                expected_size,
                yuv_data.len()
            )));
        }

        let device = self.context.device();

        // Map staging buffer.
        let data_ptr = unsafe {
            device.map_memory(
                self.staging_memory,
                0,
                self.staging_size as vk::DeviceSize,
                vk::MemoryMapFlags::empty(),
            )
        }
        .map_err(|e| PixelForgeError::MemoryAllocation(e.to_string()))?;

        unsafe {
            match self.bit_depth {
                BitDepth::Eight => {
                    // 8-bit semi-planar YUV444: Y plane, then interleaved UV plane.
                    let dst =
                        std::slice::from_raw_parts_mut(data_ptr as *mut u8, self.staging_size);

                    // Copy Y plane directly.
                    dst[..plane_size].copy_from_slice(&yuv_data[..plane_size]);

                    // Interleave U and V planes for semi-planar format.
                    let u_plane = &yuv_data[plane_size..plane_size * 2];
                    let v_plane = &yuv_data[plane_size * 2..plane_size * 3];
                    for i in 0..plane_size {
                        dst[plane_size + i * 2] = u_plane[i];
                        dst[plane_size + i * 2 + 1] = v_plane[i];
                    }
                }
                BitDepth::Ten => {
                    // 10-bit semi-planar YUV444: Expand 8-bit to 16-bit and interleave UV.
                    let dst =
                        std::slice::from_raw_parts_mut(data_ptr as *mut u16, self.staging_size / 2);

                    // Convert Y plane.
                    for i in 0..plane_size {
                        dst[i] = (yuv_data[i] as u16) << 8;
                    }
                    // Interleave and convert U and V planes.
                    let u_plane = &yuv_data[plane_size..plane_size * 2];
                    let v_plane = &yuv_data[plane_size * 2..plane_size * 3];
                    for i in 0..plane_size {
                        dst[plane_size + i * 2] = (u_plane[i] as u16) << 8;
                        dst[plane_size + i * 2 + 1] = (v_plane[i] as u16) << 8;
                    }
                }
            }

            device.unmap_memory(self.staging_memory);
        }

        // Record and submit copy commands.
        self.copy_staging_to_image_internal(target_image)?;

        Ok(())
    }

    fn copy_staging_to_image(&mut self) -> Result<()> {
        self.copy_staging_to_image_internal(self.image)
    }

    fn copy_staging_to_image_internal(&mut self, target_image: vk::Image) -> Result<()> {
        let device = self.context.device();
        let width = self.width;
        let height = self.height;

        // Calculate plane sizes based on bit depth.
        // For 8-bit: 1 byte per sample
        // For 10-bit: 2 bytes per sample (16-bit per sample)
        let bytes_per_sample = match self.bit_depth {
            BitDepth::Eight => 1,
            BitDepth::Ten => 2,
        };
        let y_plane_size_bytes = (width * height) as usize * bytes_per_sample;

        unsafe {
            device.reset_command_buffer(self.command_buffer, vk::CommandBufferResetFlags::empty())
        }
        .map_err(|e| PixelForgeError::CommandBuffer(e.to_string()))?;

        let begin_info = vk::CommandBufferBeginInfo::default()
            .flags(vk::CommandBufferUsageFlags::ONE_TIME_SUBMIT);
        unsafe { device.begin_command_buffer(self.command_buffer, &begin_info) }
            .map_err(|e| PixelForgeError::CommandBuffer(e.to_string()))?;

        // Transition image to transfer destination.
        let barrier = vk::ImageMemoryBarrier::default()
            .old_layout(vk::ImageLayout::UNDEFINED)
            .new_layout(vk::ImageLayout::TRANSFER_DST_OPTIMAL)
            .src_queue_family_index(vk::QUEUE_FAMILY_IGNORED)
            .dst_queue_family_index(vk::QUEUE_FAMILY_IGNORED)
            .image(target_image)
            .subresource_range(vk::ImageSubresourceRange {
                aspect_mask: vk::ImageAspectFlags::COLOR,
                base_mip_level: 0,
                level_count: 1,
                base_array_layer: 0,
                layer_count: 1,
            })
            .src_access_mask(vk::AccessFlags::empty())
            .dst_access_mask(vk::AccessFlags::TRANSFER_WRITE);

        unsafe {
            device.cmd_pipeline_barrier(
                self.command_buffer,
                vk::PipelineStageFlags::TOP_OF_PIPE,
                vk::PipelineStageFlags::TRANSFER,
                vk::DependencyFlags::empty(),
                &[],
                &[],
                &[barrier],
            );
        }

        // Build copy regions based on pixel format.
        let copy_regions = match self.pixel_format {
            PixelFormat::Yuv420 => {
                // NV12/P010: Y plane (PLANE_0) + interleaved UV plane (PLANE_1)
                vec![
                    // Y plane.
                    vk::BufferImageCopy::default()
                        .buffer_offset(0)
                        .buffer_row_length(0)
                        .buffer_image_height(0)
                        .image_subresource(vk::ImageSubresourceLayers {
                            aspect_mask: vk::ImageAspectFlags::PLANE_0,
                            mip_level: 0,
                            base_array_layer: 0,
                            layer_count: 1,
                        })
                        .image_offset(vk::Offset3D { x: 0, y: 0, z: 0 })
                        .image_extent(vk::Extent3D {
                            width,
                            height,
                            depth: 1,
                        }),
                    // UV plane (interleaved, half resolution).
                    vk::BufferImageCopy::default()
                        .buffer_offset(y_plane_size_bytes as vk::DeviceSize)
                        .buffer_row_length(0)
                        .buffer_image_height(0)
                        .image_subresource(vk::ImageSubresourceLayers {
                            aspect_mask: vk::ImageAspectFlags::PLANE_1,
                            mip_level: 0,
                            base_array_layer: 0,
                            layer_count: 1,
                        })
                        .image_offset(vk::Offset3D { x: 0, y: 0, z: 0 })
                        .image_extent(vk::Extent3D {
                            width: width / 2,
                            height: height / 2,
                            depth: 1,
                        }),
                ]
            }
            PixelFormat::Yuv444 => {
                // 2-plane semi-planar YUV444: Y (PLANE_0), UV interleaved (PLANE_1) at full resolution.
                vec![
                    // Y plane.
                    vk::BufferImageCopy::default()
                        .buffer_offset(0)
                        .buffer_row_length(0)
                        .buffer_image_height(0)
                        .image_subresource(vk::ImageSubresourceLayers {
                            aspect_mask: vk::ImageAspectFlags::PLANE_0,
                            mip_level: 0,
                            base_array_layer: 0,
                            layer_count: 1,
                        })
                        .image_offset(vk::Offset3D { x: 0, y: 0, z: 0 })
                        .image_extent(vk::Extent3D {
                            width,
                            height,
                            depth: 1,
                        }),
                    // UV plane (interleaved, full resolution for 444).
                    vk::BufferImageCopy::default()
                        .buffer_offset(y_plane_size_bytes as vk::DeviceSize)
                        .buffer_row_length(0)
                        .buffer_image_height(0)
                        .image_subresource(vk::ImageSubresourceLayers {
                            aspect_mask: vk::ImageAspectFlags::PLANE_1,
                            mip_level: 0,
                            base_array_layer: 0,
                            layer_count: 1,
                        })
                        .image_offset(vk::Offset3D { x: 0, y: 0, z: 0 })
                        .image_extent(vk::Extent3D {
                            width,
                            height,
                            depth: 1,
                        }),
                ]
            }
            _ => {
                return Err(PixelForgeError::InvalidInput(format!(
                    "Unsupported pixel format: {:?}",
                    self.pixel_format
                )));
            }
        };

        unsafe {
            device.cmd_copy_buffer_to_image(
                self.command_buffer,
                self.staging_buffer,
                target_image,
                vk::ImageLayout::TRANSFER_DST_OPTIMAL,
                &copy_regions,
            );
        }

        // Transition image to GENERAL (ready for encoding)
        let barrier = vk::ImageMemoryBarrier::default()
            .old_layout(vk::ImageLayout::TRANSFER_DST_OPTIMAL)
            .new_layout(vk::ImageLayout::GENERAL)
            .src_queue_family_index(vk::QUEUE_FAMILY_IGNORED)
            .dst_queue_family_index(vk::QUEUE_FAMILY_IGNORED)
            .image(target_image)
            .subresource_range(vk::ImageSubresourceRange {
                aspect_mask: vk::ImageAspectFlags::COLOR,
                base_mip_level: 0,
                level_count: 1,
                base_array_layer: 0,
                layer_count: 1,
            })
            .src_access_mask(vk::AccessFlags::TRANSFER_WRITE)
            .dst_access_mask(vk::AccessFlags::MEMORY_READ);

        unsafe {
            device.cmd_pipeline_barrier(
                self.command_buffer,
                vk::PipelineStageFlags::TRANSFER,
                vk::PipelineStageFlags::BOTTOM_OF_PIPE,
                vk::DependencyFlags::empty(),
                &[],
                &[],
                &[barrier],
            );
        }

        unsafe { device.end_command_buffer(self.command_buffer) }
            .map_err(|e| PixelForgeError::CommandBuffer(e.to_string()))?;

        // Submit and wait.
        let submit_info =
            vk::SubmitInfo::default().command_buffers(std::slice::from_ref(&self.command_buffer));

        unsafe { device.queue_submit(self.context.transfer_queue(), &[submit_info], self.fence) }
            .map_err(|e| PixelForgeError::CommandBuffer(format!("Queue submit failed: {}", e)))?;

        unsafe { device.wait_for_fences(&[self.fence], true, u64::MAX) }
            .map_err(|e| PixelForgeError::Synchronization(e.to_string()))?;

        unsafe { device.reset_fences(&[self.fence]) }
            .map_err(|e| PixelForgeError::Synchronization(e.to_string()))?;

        Ok(())
    }

    /// Get the underlying Vulkan image handle.
    pub fn image(&self) -> vk::Image {
        self.image
    }

    /// Get the image dimensions.
    pub fn dimensions(&self) -> (u32, u32) {
        (self.width, self.height)
    }
}

impl Drop for InputImage {
    fn drop(&mut self) {
        let device = self.context.device();
        unsafe {
            device.destroy_fence(self.fence, None);
            device.destroy_command_pool(self.command_pool, None);
            device.destroy_buffer(self.staging_buffer, None);
            device.free_memory(self.staging_memory, None);
            device.destroy_image(self.image, None);
            device.free_memory(self.memory, None);
        }
    }
}
